{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Homework3_SHedaya.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_l6pomKblMV",
        "colab_type": "text"
      },
      "source": [
        "# Home 3: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Sarita Hedaya\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI7TLaU6blMX",
        "colab_type": "text"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run the code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "    \n",
        "4. Upload this .HTML file to your Google Drive, Dropbox, or Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583-2019F/blob/master/homework/HM3/HM3.html\n",
        "\n",
        "\n",
        "## Requirements:\n",
        "\n",
        "1. You can use whatever CNN architecture, including VGG, Inception, and ResNet. However, you must build the networks layer by layer. You must NOT import the archetectures from ```keras.applications```.\n",
        "\n",
        "2. Make sure ```BatchNormalization``` is between a ```Conv```/```Dense``` layer and an ```activation``` layer.\n",
        "\n",
        "3. If you want to regularize a ```Conv```/```Dense``` layer, you should place a ```Dropout``` layer **before** the ```Conv```/```Dense``` layer.\n",
        "\n",
        "4. An accuracy above 70% is considered reasonable. An accuracy above 80% is considered good. Without data augmentation, achieving 80% accuracy is difficult.\n",
        "\n",
        "\n",
        "## Google Colab\n",
        "\n",
        "- If you do not have GPU, the training of a CNN can be slow. Google Colab is a good option.\n",
        "\n",
        "- Keep in mind that you must download it as an IPYNB file and then use IPython Notebook to convert it to HTML.\n",
        "\n",
        "- Also keep in mind that the IPYNB and HTML files must contain the outputs. (Otherwise, the instructor will not be able to know the correctness and performance.) Do the followings to keep the outputs.\n",
        "\n",
        "- In Colab, go to ```Runtime``` --> ```Change runtime type``` --> Do NOT check ```Omit code cell output when saving this notebook```. In this way, the downloaded IPYNB file contains the outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0wemXwXblMX",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyK7ZqwqblMY",
        "colab_type": "text"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn9TiFIOblMZ",
        "colab_type": "code",
        "outputId": "0a47d888-a027-4727-e700-20f330587ef4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJOfq8-CrGcf",
        "colab_type": "text"
      },
      "source": [
        "### How does the data look like?\n",
        "\n",
        "Print a data point to see how it looks like. This will help me know if I need to rescale to 1/255"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-hLMa-VrXTu",
        "colab_type": "code",
        "outputId": "bcc5f661-6696-4edb-91b8-b41262a6bde3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "print(x_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[ 59  62  63]\n",
            "  [ 43  46  45]\n",
            "  [ 50  48  43]\n",
            "  ...\n",
            "  [158 132 108]\n",
            "  [152 125 102]\n",
            "  [148 124 103]]\n",
            "\n",
            " [[ 16  20  20]\n",
            "  [  0   0   0]\n",
            "  [ 18   8   0]\n",
            "  ...\n",
            "  [123  88  55]\n",
            "  [119  83  50]\n",
            "  [122  87  57]]\n",
            "\n",
            " [[ 25  24  21]\n",
            "  [ 16   7   0]\n",
            "  [ 49  27   8]\n",
            "  ...\n",
            "  [118  84  50]\n",
            "  [120  84  50]\n",
            "  [109  73  42]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[208 170  96]\n",
            "  [201 153  34]\n",
            "  [198 161  26]\n",
            "  ...\n",
            "  [160 133  70]\n",
            "  [ 56  31   7]\n",
            "  [ 53  34  20]]\n",
            "\n",
            " [[180 139  96]\n",
            "  [173 123  42]\n",
            "  [186 144  30]\n",
            "  ...\n",
            "  [184 148  94]\n",
            "  [ 97  62  34]\n",
            "  [ 83  53  34]]\n",
            "\n",
            " [[177 144 116]\n",
            "  [168 129  94]\n",
            "  [179 142  87]\n",
            "  ...\n",
            "  [216 184 140]\n",
            "  [151 118  84]\n",
            "  [123  92  72]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6DiY-IjblMc",
        "colab_type": "text"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bu6YIXgrblMc",
        "colab_type": "code",
        "outputId": "ff4e2b26-5523-44a7-d517-81fe8de8c0bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    one_hot_labels = numpy.zeros((y.shape[0], num_class))\n",
        "    for idx, label in enumerate(y):\n",
        "      one_hot_labels[idx, label] = 1\n",
        "    return one_hot_labels\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBqyvElWblMe",
        "colab_type": "text"
      },
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52qC7EmKkgFA",
        "colab_type": "text"
      },
      "source": [
        "### Normalize Data\n",
        "\n",
        "For some reason, resizing with data augmentation would completely ruin my training and validation loss.\n",
        "\n",
        "This custom function to resize pixels did it without an issue. Im not sure resizing is necessary, but seems to be common practice. So I did it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bY8QWNoFkfa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# scale pixels\n",
        "def scale_pixels(train, test):\n",
        "\t# convert from integers to floats first\n",
        "\ttrain_norm = train.astype('float32')\n",
        "\ttest_norm = test.astype('float32')\n",
        "\t# normalize to range 0-1\n",
        "\ttrain_norm = train_norm / 255.0\n",
        "\ttest_norm = test_norm / 255.0\n",
        "\t# return normalized images\n",
        "\treturn train_norm, test_norm\n",
        "\n",
        "x_train, x_test = scale_pixels(x_train, x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M41QLmYblMf",
        "colab_type": "text"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LGnvQaublMg",
        "colab_type": "code",
        "outputId": "1ded46d1-0821-4254-ab4a-0778029a9f8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hua3_DAZblMi",
        "colab_type": "text"
      },
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynU-5akvblMj",
        "colab_type": "text"
      },
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzARmk4Kn-y6",
        "colab_type": "text"
      },
      "source": [
        "### Define the NN Architecture\n",
        "\n",
        "**A Note on how I got here**\n",
        "\n",
        "> This is the network architecture I arrived at after trying over 15 different approaches over a period of one week. All of my attempts are documented in a google doc. I didn't upload it because its pretty messy :p\n",
        "\n",
        "\n",
        "\n",
        "*   Starting with the given network, and adding layers and dropout, I quickly realized that validation accuracy wouldnt go above the low 70's without data augmentation. \n",
        "\n",
        "*   I implemented data augmentation and tuned it a bit, since then I experienced almost no overfitting (except on a few experiments).\n",
        "\n",
        "* I continuously increased the complexity of my model, from under 1M parameters, to well above 3M parameters, seeing no overfitting when using light dropout (0.1-0.3 for the conv layers--which I later realized was a mistake, 0.4 for the FC).\n",
        "\n",
        "* At one point I was stuck, not being able to reach above ~81.5% validation accuracy. Given that I had experienced nearly no overfitting, and that my intuition told me that conv layer dropout made little sense, I decided to remove the dropout on the convolutional layers. \n",
        "\n",
        "* Seeing that my accuracy increased, I did some research and turns out Dropout in Conv layers is a bad idea.\n",
        "\n",
        "* Below is the final model. I'm sure it can be improved with fancier architectures, but given that each epoch takes almost two minutes to train, I left it at that. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Tk0THpfo--",
        "colab_type": "code",
        "outputId": "9e60b99d-8991-4f8d-f437-f45c423670eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Activation\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dropout\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (7,7), padding='same', input_shape=(32, 32, 3)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(32, (7,7), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(32, (7,7), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2)) # is dropout useful here?\n",
        "\n",
        "model.add(Conv2D(64, (5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(Conv2D(64, (5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64, (5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(64, (5,5), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.1))\n",
        " \n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.1)) #see if this will help. half million params\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Dropout(0.1)) #see if this will help. half million params\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Dropout(0.1)) #see if this will help. half million params\n",
        "\n",
        "model.add(Conv2D(256, (3,3), padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "# model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "# model.add(Dropout(0.2))\n",
        " \n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        " \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_127 (Conv2D)          (None, 32, 32, 32)        4736      \n",
            "_________________________________________________________________\n",
            "batch_normalization_127 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_127 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_128 (Conv2D)          (None, 32, 32, 32)        50208     \n",
            "_________________________________________________________________\n",
            "batch_normalization_128 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_128 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_129 (Conv2D)          (None, 32, 32, 32)        50208     \n",
            "_________________________________________________________________\n",
            "batch_normalization_129 (Bat (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "activation_129 (Activation)  (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_29 (MaxPooling (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_130 (Conv2D)          (None, 16, 16, 64)        51264     \n",
            "_________________________________________________________________\n",
            "batch_normalization_130 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_130 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_131 (Conv2D)          (None, 16, 16, 64)        102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization_131 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_131 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_132 (Conv2D)          (None, 16, 16, 64)        102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization_132 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_132 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_133 (Conv2D)          (None, 16, 16, 64)        102464    \n",
            "_________________________________________________________________\n",
            "batch_normalization_133 (Bat (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_133 (Activation)  (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_30 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_134 (Conv2D)          (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_134 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_134 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_135 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_135 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_135 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_136 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_136 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_136 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_137 (Conv2D)          (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_137 (Bat (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_137 (Activation)  (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_31 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_41 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_138 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_138 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_138 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_139 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_139 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_139 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_140 (Conv2D)          (None, 4, 4, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_140 (Bat (None, 4, 4, 128)         512       \n",
            "_________________________________________________________________\n",
            "activation_140 (Activation)  (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_32 (MaxPooling (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_42 (Dropout)         (None, 2, 2, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_141 (Conv2D)          (None, 2, 2, 256)         295168    \n",
            "_________________________________________________________________\n",
            "batch_normalization_141 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_141 (Activation)  (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_43 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_142 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_142 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_142 (Activation)  (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_44 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_143 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_143 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_143 (Activation)  (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_45 (Dropout)         (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_144 (Conv2D)          (None, 2, 2, 256)         590080    \n",
            "_________________________________________________________________\n",
            "batch_normalization_144 (Bat (None, 2, 2, 256)         1024      \n",
            "_________________________________________________________________\n",
            "activation_144 (Activation)  (None, 2, 2, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_8 (Flatten)          (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dropout_46 (Dropout)         (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 128)               131200    \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 3,630,154\n",
            "Trainable params: 3,625,610\n",
            "Non-trainable params: 4,544\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql7sv_MC5v79",
        "colab_type": "text"
      },
      "source": [
        "### Compile model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8srQpD7ublMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 0.001 # from 0.00001 to 0.0001: val_acc stops increasing at epoch 10 - 0.66\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-DKWV1IpY28",
        "colab_type": "text"
      },
      "source": [
        "### Reduce the Learning Rate during training\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKRZcEOnJM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
        "                               cooldown=0, patience=3, min_lr=0.5e-6, verbose=1)\n",
        "\n",
        "\n",
        "\n",
        "csv_logger = (CSVLogger(\"/content/log.csv\"))\n",
        "\n",
        "early_stopper = EarlyStopping(min_delta=0.001, patience=12, verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"/content/model_final.hdf5\", monitor=\"val_loss\", verbose=1, save_best_only=True, mode=\"auto\", period=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ntIuZfwoBsX",
        "colab_type": "text"
      },
      "source": [
        "### Use data augmentation\n",
        "\n",
        "The objects on CIFAR10 can be flipped horizontally and still make sense. \n",
        "\n",
        "They can also be rotated slightly and shifted vertically and horizontally and still make sense.\n",
        "\n",
        "Use the ImageDataGenerator to generate samples using random combinations of these characteristics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5egVWTSOnVnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "image_generator = ImageDataGenerator(\n",
        "                                    # rescale=1./255, \n",
        "                                    horizontal_flip = True, \n",
        "                                    rotation_range = 10, \n",
        "                                    height_shift_range = 0.05, \n",
        "                                     width_shift_range = 0.05)\n",
        "\n",
        "image_generator.fit(x_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ir27JwjyblMo",
        "colab_type": "code",
        "outputId": "9e1c068e-e937-4bab-e53a-4dbd6bfc722d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit_generator(image_generator.flow(x_tr, y_tr, batch_size=32),\n",
        "                              steps_per_epoch = x_tr.shape[0] // 32,\n",
        "                              epochs=70, \n",
        "                              validation_data=(x_val, y_val),\n",
        "                              callbacks = [lr_reducer, csv_logger, checkpoint, early_stopper],\n",
        "                              verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/70\n",
            "1250/1250 [==============================] - 99s 79ms/step - loss: 1.9251 - acc: 0.2754 - val_loss: 1.7446 - val_acc: 0.3403\n",
            "Epoch 2/70\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 1.5945 - acc: 0.4150 - val_loss: 1.5851 - val_acc: 0.4210\n",
            "Epoch 3/70\n",
            "1250/1250 [==============================] - 92s 74ms/step - loss: 1.4174 - acc: 0.4862 - val_loss: 1.6172 - val_acc: 0.4351\n",
            "Epoch 4/70\n",
            "1250/1250 [==============================] - 92s 74ms/step - loss: 1.2806 - acc: 0.5425 - val_loss: 1.3030 - val_acc: 0.5215\n",
            "Epoch 5/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 1.1640 - acc: 0.5846 - val_loss: 1.3671 - val_acc: 0.5200\n",
            "\n",
            "Epoch 00005: val_loss improved from inf to 1.36706, saving model to /content/model_custom_prep_pixel_3convperlayer.hdf5\n",
            "Epoch 6/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 1.0640 - acc: 0.6237 - val_loss: 1.3580 - val_acc: 0.5422\n",
            "Epoch 7/70\n",
            "1250/1250 [==============================] - 92s 74ms/step - loss: 0.9792 - acc: 0.6569 - val_loss: 0.9258 - val_acc: 0.6711\n",
            "Epoch 8/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.9030 - acc: 0.6829 - val_loss: 1.0428 - val_acc: 0.6440\n",
            "Epoch 9/70\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.8434 - acc: 0.7066 - val_loss: 0.8918 - val_acc: 0.6909\n",
            "Epoch 10/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.7866 - acc: 0.7269 - val_loss: 0.9282 - val_acc: 0.6913\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.36706 to 0.92821, saving model to /content/model_custom_prep_pixel_3convperlayer.hdf5\n",
            "Epoch 11/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7479 - acc: 0.7393 - val_loss: 1.1657 - val_acc: 0.6383\n",
            "Epoch 12/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.7059 - acc: 0.7567 - val_loss: 0.7527 - val_acc: 0.7434\n",
            "Epoch 13/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.6654 - acc: 0.7682 - val_loss: 0.9237 - val_acc: 0.6876\n",
            "Epoch 14/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.6332 - acc: 0.7827 - val_loss: 0.7533 - val_acc: 0.7389\n",
            "Epoch 15/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.6046 - acc: 0.7924 - val_loss: 0.9563 - val_acc: 0.6957\n",
            "\n",
            "Epoch 00015: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.92821\n",
            "Epoch 16/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4983 - acc: 0.8288 - val_loss: 0.5702 - val_acc: 0.8081\n",
            "Epoch 17/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.4642 - acc: 0.8395 - val_loss: 0.5781 - val_acc: 0.8067\n",
            "Epoch 18/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4543 - acc: 0.8432 - val_loss: 0.5713 - val_acc: 0.8085\n",
            "Epoch 19/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4366 - acc: 0.8479 - val_loss: 0.5605 - val_acc: 0.8123\n",
            "Epoch 20/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4300 - acc: 0.8529 - val_loss: 0.5623 - val_acc: 0.8107\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.92821 to 0.56225, saving model to /content/model_custom_prep_pixel_3convperlayer.hdf5\n",
            "Epoch 21/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4270 - acc: 0.8537 - val_loss: 0.5549 - val_acc: 0.8138\n",
            "Epoch 22/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4130 - acc: 0.8585 - val_loss: 0.5452 - val_acc: 0.8176\n",
            "Epoch 23/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.4102 - acc: 0.8580 - val_loss: 0.5687 - val_acc: 0.8119\n",
            "Epoch 24/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3963 - acc: 0.8628 - val_loss: 0.5517 - val_acc: 0.8180\n",
            "Epoch 25/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3969 - acc: 0.8617 - val_loss: 0.5542 - val_acc: 0.8181\n",
            "\n",
            "Epoch 00025: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.56225 to 0.55416, saving model to /content/model_custom_prep_pixel_3convperlayer.hdf5\n",
            "Epoch 26/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3769 - acc: 0.8703 - val_loss: 0.5483 - val_acc: 0.8191\n",
            "Epoch 27/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3739 - acc: 0.8717 - val_loss: 0.5475 - val_acc: 0.8210\n",
            "Epoch 28/70\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.3697 - acc: 0.8716 - val_loss: 0.5436 - val_acc: 0.8212\n",
            "Epoch 29/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3688 - acc: 0.8729 - val_loss: 0.5408 - val_acc: 0.8226\n",
            "Epoch 30/70\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.3718 - acc: 0.8702 - val_loss: 0.5412 - val_acc: 0.8220\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.55416 to 0.54123, saving model to /content/model_custom_prep_pixel_3convperlayer.hdf5\n",
            "Epoch 31/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3638 - acc: 0.8738 - val_loss: 0.5428 - val_acc: 0.8229\n",
            "Epoch 32/70\n",
            "1250/1250 [==============================] - 94s 75ms/step - loss: 0.3667 - acc: 0.8725 - val_loss: 0.5437 - val_acc: 0.8225\n",
            "\n",
            "Epoch 00032: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
            "Epoch 33/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3665 - acc: 0.8736 - val_loss: 0.5467 - val_acc: 0.8212\n",
            "Epoch 34/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3640 - acc: 0.8738 - val_loss: 0.5459 - val_acc: 0.8207\n",
            "Epoch 35/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3622 - acc: 0.8734 - val_loss: 0.5452 - val_acc: 0.8217\n",
            "\n",
            "Epoch 00035: ReduceLROnPlateau reducing learning rate to 5e-07.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.54123\n",
            "Epoch 36/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3634 - acc: 0.8752 - val_loss: 0.5462 - val_acc: 0.8211\n",
            "Epoch 37/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3644 - acc: 0.8734 - val_loss: 0.5447 - val_acc: 0.8212\n",
            "Epoch 38/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3669 - acc: 0.8722 - val_loss: 0.5469 - val_acc: 0.8208\n",
            "Epoch 39/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3629 - acc: 0.8738 - val_loss: 0.5486 - val_acc: 0.8212\n",
            "Epoch 40/70\n",
            "1250/1250 [==============================] - 93s 74ms/step - loss: 0.3633 - acc: 0.8745 - val_loss: 0.5481 - val_acc: 0.8205\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.54123\n",
            "Epoch 41/70\n",
            "1250/1250 [==============================] - 93s 75ms/step - loss: 0.3667 - acc: 0.8731 - val_loss: 0.5457 - val_acc: 0.8214\n",
            "Epoch 00041: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO1oSZQ06CLI",
        "colab_type": "text"
      },
      "source": [
        "### Make some visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yU5_5srGblMq",
        "colab_type": "code",
        "outputId": "b29b6811-67e2-4e76-d095-f5318025bd0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEJCAYAAACZjSCSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzv1/ZHLZOQDaCRUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxhiChQ_RjPW",
        "colab_type": "code",
        "outputId": "c05f3c93-ccb8-4815-de9f-1ae6c7889f37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        }
      },
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, train_loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiGJ5NqQblMv",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KU5FYLyiblMw",
        "colab_type": "text"
      },
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FJeZYAK3IWiD",
        "colab": {}
      },
      "source": [
        "from keras import optimizers\n",
        "\n",
        "learning_rate = 0.001 \n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(lr=learning_rate, momentum=0.9),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "411f5199-e958-4bad-9027-e35008542af5",
        "id": "juilJXS0IcDS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "history = model.fit_generator(image_generator.flow(x_train, y_train_vec, batch_size=32),\n",
        "                              steps_per_epoch = x_train.shape[0] // 32,\n",
        "                              epochs=35)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/35\n",
            " 141/1562 [=>............................] - ETA: 1:45 - loss: 0.3656 - acc: 0.8732"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPNGP_LCblM3",
        "colab_type": "text"
      },
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy_z0akLblM4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d72803e6-5c95-43b0-851c-4a7ad3796ca4"
      },
      "source": [
        "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 6s 612us/step\n",
            "loss = 0.5192452391147614\n",
            "accuracy = 0.8306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2nCMvA3Wskv",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}